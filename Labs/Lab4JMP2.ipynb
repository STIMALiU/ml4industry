{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4, Reinforcement Learning\n",
    "\n",
    "Machine Learning for Industry, Linköping University, Fall 2019.\n",
    "\n",
    "Author: [Joel Oskarsson](mailto:joeos014@student.liu.se), Linköping University\n",
    "\n",
    "## Introduction\n",
    "The purpose of this lab is to get some practical experience with reinforcement learning and understand how an agent can learn from acting in an environment. \n",
    "Your will implement key parts of the Q-learning and REINFORCE algorithms. Details on these algorithms can be found in the lecture slides and the book by Sutton and Barton.\n",
    "\n",
    "## Agent and Environment\n",
    "The first step when applying reinforcement learning to a problem is to clearly define the environment, possible actions and rewards. \n",
    "In this lab we will work with a grid-world environment consisting of $H$ x $W$ tiles laid out in a 2-dimensional grid, as can be seen in figure 1.\n",
    "<center>\n",
    "    <img width=\"350\" src=\"https://gitlab.liu.se/joeos014/figures/raw/master/rl/environment.png\">\n",
    "    <br>\n",
    "    Figure 1: The grid-world environment\n",
    "</center>\n",
    "\n",
    "The agent can act by moving up, down, left or right. As a Markov decision process this is:\n",
    "\n",
    "* State space: $\\mathcal{S} = \\left\\{(y,x) | y \\in \\left\\{0,1,\\dots,H-1\\right\\}, x \\in \\left\\{0,1,\\dots,W-1\\right\\}\\right\\}$\n",
    "* Action space: $\\mathcal{A} = \\left\\{\\text{up}, \\text{down}, \\text{left}, \\text{right}\\right\\}$\n",
    "\n",
    "Additionally:\n",
    "* We assume the state space to be fully observable.\n",
    "* The reward function is a deterministic function of the state and does not depend on the actions taken by the agent. Assume the agent gets the reward as soon as it moves to a state.\n",
    "* The transition model is defined by the agent moving in the direction chosen with probability $(1-\\beta)$. The agent might also slip and end up moving in the direction to the left or right of its chosen action, each with a probability $\\beta/2$. This is illustrated in figure 2.\n",
    "<center>\n",
    "    <img width=\"350\" src=\"https://gitlab.liu.se/joeos014/figures/raw/master/rl/transition_model.png\">\n",
    "    <br>\n",
    "    Figure 2: The transition model\n",
    "</center>\n",
    "\n",
    "* The transition model is unknown to the agent, forcing us to resort to model-free approaches.\n",
    "* The environment is episodic and all states with a non-zero reward are terminal (each episode ends when the agent receives **any** reward).\n",
    "* Throughout this lab we are going to need integer representations of the different actions. For consistency we always use the following mapping: **{up: 0, right: 1, down: 2, left: 3}**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. We start by importing some neccesary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as coll\n",
    "import matplotlib.patches as patch\n",
    "\n",
    "import scipy.linalg as lin\n",
    "from keras import models, layers, optimizers, utils\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is some package that you have not used before it should be straightforward to install it using pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment A\n",
    "For our first environment we will choose $H=5$, $W=7$. \n",
    "This environment includes a reward of 10 in state $(2,5)$ and a reward of -1 in each of the states $(1,2), (2,2), (3,2)$.\n",
    "We specify the rewards using a reward map in the form of a matrix with one entry for each state. States with no reward will simply have a matrix entry of 0.\n",
    "The agent starts each episode in the state $(2,0)$ and for the transition model we set $\\beta = 0$, so all transitions are completely deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 5\n",
    "W = 7\n",
    "\n",
    "reward_map = np.zeros(shape=(H,W))\n",
    "reward_map[2,5] = 10.\n",
    "reward_map[1:4,2] = -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <tt>vis_environment</tt> can be used to visualize the environment and learned policy. You will not have to modify this function, but read the docstring to familiarize yourself with how it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 0.9\n",
    "ARROWS = [\"↑\", \"→\", \"↓\", \"←\"]\n",
    "\n",
    "def vis_environment(reward_map, q_table=None, policy=None, title=None):\n",
    "    \"\"\"\n",
    "    Visualize an environment with rewards. \n",
    "    If a Q-table is given the Q-values for all actions are displayed on the edges of each tile.\n",
    "    If a policy is given also visualize the chosen action for each state.\n",
    "    \n",
    "    Args:\n",
    "        reward_map: a HxW numpy array containing the reward given at each state\n",
    "        q_table (optional): a HxWx4 numpy array containing Q-values for each state-action pair\n",
    "        policy (optional): policy object that implements an act method\n",
    "        title (optional): figure title\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "    \n",
    "    tiles = []\n",
    "    size_y, size_x = reward_map.shape\n",
    "    \n",
    "    max_val = np.max(reward_map)\n",
    "    min_val = np.min(reward_map)\n",
    "    \n",
    "    if not (q_table is None):\n",
    "        max_val = max(max_val, np.max(q_table))\n",
    "        min_val = min(min_val, np.min(np.max(q_table, axis=2)))\n",
    "    \n",
    "    for i_x in range(size_x): \n",
    "        for i_y in range(size_y): \n",
    "            x = i_x - 0.5*TILE_SIZE\n",
    "            y = i_y - 0.5*TILE_SIZE\n",
    "            \n",
    "            tile_value = 0.\n",
    "            tile_reward = reward_map[i_y, i_x]\n",
    "            if tile_reward != 0.:\n",
    "                tile_value = tile_reward\n",
    "                \n",
    "                # Write out reward\n",
    "                ax.text(i_x, i_y, s=\"R={:.2f}\".format(tile_reward), \n",
    "                        verticalalignment=\"center\", horizontalalignment=\"center\", fontsize=12) # reward\n",
    "            elif not (q_table is None):\n",
    "                tile_value = np.max(q_table[i_y,i_x])\n",
    "                \n",
    "                # Direction Q-values\n",
    "                ax.text(i_x, i_y-0.5*TILE_SIZE, s=\"{:.2f}\".format(q_table[i_y,i_x,0]), \n",
    "                        verticalalignment=\"top\", horizontalalignment=\"center\", fontsize=12) # up\n",
    "                ax.text(i_x, i_y+0.5*TILE_SIZE, s=\"{:.2f}\".format(q_table[i_y,i_x,2]), \n",
    "                        verticalalignment=\"bottom\", horizontalalignment=\"center\", fontsize=12) # down\n",
    "                ax.text(i_x-0.5*TILE_SIZE, i_y, s=\"{:.2f}\".format(q_table[i_y,i_x,3]), \n",
    "                        verticalalignment=\"center\", horizontalalignment=\"left\", fontsize=12) # left\n",
    "                ax.text(i_x+0.5*TILE_SIZE, i_y, s=\"{:.2f}\".format(q_table[i_y,i_x,1]), \n",
    "                        verticalalignment=\"center\", horizontalalignment=\"right\", fontsize=12) # right\n",
    "            \n",
    "            # Set tile color based on value\n",
    "            if tile_value >= 0:\n",
    "                color_scale = tile_value/max_val if max_val > 0 else 0.0\n",
    "                tile_color = (1.0-color_scale,1.0,1.0-color_scale)\n",
    "            else:\n",
    "                color_scale = abs(tile_value/min_val)\n",
    "                tile_color = (1.0,1.0-color_scale,1.0-color_scale)\n",
    "            \n",
    "            tiles.append(patch.Rectangle((x,y),TILE_SIZE,TILE_SIZE, facecolor=tile_color))\n",
    "        \n",
    "            \n",
    "            if policy and (tile_reward == 0.):\n",
    "                # Show policy decisions\n",
    "                policy_action = policy.act(i_y, i_x)\n",
    "                ax.text(i_x, i_y, s=ARROWS[policy_action], verticalalignment=\"center\", \n",
    "                        horizontalalignment=\"center\", fontsize=30)\n",
    "    \n",
    "    tile_coll = coll.PatchCollection(tiles, match_original=True, edgecolor=\"#787878\")\n",
    "    ax.add_collection(tile_coll)\n",
    "    ax.set_xlim(-0.5, size_x-0.5)\n",
    "    ax.set_ylim(-0.5, size_y-0.5)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title, fontsize=15)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:orange;margin-bottom:0px;\">Task 0.1: Visualize Rewards</h2>\n",
    "\n",
    "Use the <tt>vis_environment</tt> function to visualize the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_environment(reward_map, title=\"Rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Q-learning\n",
    "We will now use Q-learning to make the agent learn a policy for acting in this environment.\n",
    "Recall that the Q-learning update is defined as:\n",
    "$$\n",
    "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left(R_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}} Q_t(S_{t+1},a) - Q(S_t,A_t) \\right) \n",
    "$$\n",
    "for taking the action $A_t$ in state $S_t$, ending up at state $S_{t+1}$ and receiving reward $R_{t+1}$.\n",
    "Unless stated otherwise we will use the values $\\alpha = 0.1$ and $\\gamma=0.95$ throughout this lab.\n",
    "\n",
    "When implementing Q-learning the estimated values of $Q(S,A)$ are commonly stored in a data-structured called Q-table. \n",
    "This is nothing but a tensor with one entry for each state-action pair.\n",
    "Since we have a $H$ x $W$ environment with 4 actions we can use a 3D-tensor of dimensions $H$ x $W$ x 4 to represent our Q-table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:orange;margin-bottom:0px;\">Task 1.1: Define Q-table</h2>\n",
    "\n",
    "* Define the Q-table as a numpy array with the shape described above. Initialize all Q-values to 0.\n",
    "* Use the <tt>vis_environment</tt> function to visualize the initial state of the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_table = np.zeros(shape=(H,W,4))\n",
    "vis_environment(reward_map, q_table=q_table, title=\"Initial Q-Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define some policies we are going to need. Since Q-learning is an off-policy algorithm we will use an $\\epsilon$-greedy exploration policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:orange;margin-bottom:0px;\">Task 1.2: Implement Policies</h2>\n",
    "\n",
    "$\\newcommand{\\argmax}{\\mathop{\\mathrm{argmax}}}$\n",
    "Below you will find class definitions for classes implementing a greedy policy and an $\\epsilon$-greedy policy. Your task is to implement the <tt>act</tt> methods of these classes.\n",
    "\n",
    "* Implement the <tt>act</tt> method of <tt>GreedyPolicy</tt> to act greedily with respect to the Q-table. In some states there might be multiple actions with the same Q-value. To speed up training it is a good idea to then sample uniformly from the set $\\argmax_{a \\in \\mathcal{A}} Q(S,a)$.\n",
    "* Implement the <tt>act</tt> method of <tt>EpsilonGreedyPolicy</tt>. Recall that an $\\epsilon$-greedy policy acts randomly with probability $\\epsilon$ and else acts greedily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy:\n",
    "    \"\"\"\n",
    "    Policy for acting greedily with respect to a Q-table.\n",
    "    \"\"\"\n",
    "    def __init__(self, q_table):\n",
    "        \"\"\"\n",
    "        Initialize a new GreedyPolicy acting with respect to given Q-table.\n",
    "        \n",
    "        Args:\n",
    "            q_table: Q-table to derive greedy policy from\n",
    "        \"\"\"\n",
    "        self.q = q_table\n",
    "        \n",
    "    def act(self, y, x):\n",
    "        \"\"\"\n",
    "        Get a greedy action for state (y,x) from the policy.\n",
    "        Args:\n",
    "            y: state y coordinate\n",
    "            x: state x coordinate\n",
    "            \n",
    "        Returns:\n",
    "             An action (integer in {0,1,2,3}) according to the policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        # Recall to sample uniformly from argmax set\n",
    "        \n",
    "        Q_s = self.q[y,x]\n",
    "        argmax = np.argwhere(Q_s == np.amax(Q_s))[:,0]\n",
    "        argmax_size = argmax.shape[0]\n",
    "\n",
    "        if argmax_size == 1:\n",
    "            return argmax[0]\n",
    "        else:\n",
    "            return argmax[np.random.randint(0,argmax_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(GreedyPolicy):\n",
    "    \"\"\"\n",
    "    Policy for acting epsilon-greedily with respect to a Q-table.\n",
    "    \"\"\"\n",
    "    def __init__(self, q_table, epsilon):\n",
    "        \"\"\"\n",
    "        Initialize a new EpsilonGreedyPolicy.\n",
    "        \n",
    "        Args:\n",
    "            q_table: Q-table to derive greedy policy from\n",
    "            epsilon: probability to act randomly\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        super(EpsilonGreedyPolicy, self).__init__(q_table)\n",
    "        \n",
    "    def act(self, y, x):\n",
    "        \"\"\"\n",
    "        Get an action for state (y,x) from the policy .\n",
    "        Args:\n",
    "            y: state y coordinate\n",
    "            x: state x coordinate\n",
    "            \n",
    "        Returns:\n",
    "             An action (integer in {0,1,2,3}) according to the policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        # You can use super(EpsilonGreedyPolicy, self).act(y, x) to call act from GreedyPolicy\n",
    "        \n",
    "        trial = np.random.binomial(1, self.epsilon)\n",
    "        \n",
    "        if trial == 1:\n",
    "            # Act randomly\n",
    "            return np.random.randint(0, 4)\n",
    "        else:\n",
    "            # Act greedily\n",
    "            return super(EpsilonGreedyPolicy, self).act(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a policy to use for exploration, let's implement the main Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:orange;margin-bottom:0px;\">Task 1.3: Implement Q-learning</h2>\n",
    "\n",
    "The function <tt>q_learning</tt> should run one episode of the agent acting in the environment and update the Q-table accordingly. A transition model taking $\\beta$ as input is implemented for you in the function <tt>transition</tt>.\n",
    "\n",
    "* Implement the transition to new states using the given exploration policy and transition model.\n",
    "* Implement the Q-table update based on the Q-learning update equation.\n",
    "* Make the <tt>q_learning</tt> function return the episode reward and the sum of the temporal-difference correction terms $\\left(R_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}} Q_t(S_{t+1},a) - Q(S_t,A_t) \\right)$ for all steps in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful constants\n",
    "ACTION_DELTAS = np.array([\n",
    "    [-1, 0], # up\n",
    "    [0, 1], # right\n",
    "    [1, 0], # down\n",
    "    [0, -1], # left\n",
    "])\n",
    "\n",
    "def transition_model(state, action, beta, size_y, size_x):\n",
    "    \"\"\"\n",
    "    Computes the new state after given action is taken. \n",
    "    The agent will follow the action with probability (1-beta) and slip ot the right or left with probability beta/2 each.\n",
    "\n",
    "    Args:\n",
    "        state: the state (y,x) the agent is currently in\n",
    "        action: which action the agent takes (in {0,1,2,3})\n",
    "        beta: probability of the agent slipping to the side when trying to move\n",
    "        size_y: size of the state space in the y-dimension \n",
    "        size_x: size of the state space in the x-dimension \n",
    "\n",
    "    Returns:\n",
    "        The new state after the action has been taken.\n",
    "    \"\"\"\n",
    "    # Sample from categorical distribution\n",
    "    delta = np.random.choice([-1, 0, 1], p=[0.5*beta, (1-beta), 0.5*beta])\n",
    "    final_action = (action + delta + 4) % 4\n",
    "\n",
    "    new_state = state + ACTION_DELTAS[final_action]\n",
    "    new_state = np.clip(new_state, 0, [size_y-1, size_x-1]) # Clip to environment size\n",
    "    return new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(q_table, reward_map, exploration_policy, start_state, alpha=0.1, gamma=0.95, beta=0.0):\n",
    "    \"\"\"\n",
    "    Perform one episode of Q-learning. The agent should move around in the \n",
    "    environment using the given transition model and update the Q-table.\n",
    "    The episode ends when the agent reaches a terminal state.\n",
    "    \n",
    "    Args:\n",
    "        q_table: Q-table to update Q-values in\n",
    "        reward_map: matrix containing the reward for each state in the state space\n",
    "        exploration_policy: object of the policy class implementing an act-method to be used for exploring the state space\n",
    "        start_state: numpy array with two entries, describing the starting position of the agent\n",
    "        alpha (optional): learning rate\n",
    "        gamma (optional): discount factor\n",
    "        beta (optional): slipping factor\n",
    "        \n",
    "    Returns:\n",
    "        reward: reward received in the episode\n",
    "        correction: temporal difference correction term\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:orange;margin-bottom:0px;\">Task 1.4: Learn !</h2>\n",
    "\n",
    "* Run 1000 episodes of Q-learning. Use an $\\epsilon$-greedy policy with $\\epsilon=0.5$ for exploration.  \n",
    "* After episode 10, 100 and 1000, visualize the Q-table and a greedy policy derived from it (use the <tt>vis_environment</tt> function).\n",
    "* Answer the questions below:\n",
    "    1. What has the agent learned after the first 10 episodes?\n",
    "    2. Is the final greedy policy (after 1000 episodes) optimal? Why / Why not?\n",
    "    3. Does the agent learn that there are multiple paths to get to the positive reward? If not, what could be done to make the agent learn this ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros(shape=(H,W,4))\n",
    "\n",
    "# Define policies\n",
    "expl_policy = EpsilonGreedyPolicy(q_table, 0.5)\n",
    "greedy_policy = GreedyPolicy(q_table)\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "Your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: New Environments\n",
    "In this section, we consider some new environments and investigate the impact of changing different parameters. All environments still fit within the previous grid-world definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment B\n",
    "This is a $7 \\times 8$ environment where the top and bottom rows have a negative reward. \n",
    "In this environment the agent starts each episode in the state (3, 0) and the transition model is deterministic ($\\beta = 0$).\n",
    "There are two positive rewards, of 5 and 10. \n",
    "The reward of 5 is easily reachable, but the agent has to navigate around the first reward in order to find the reward worth 10. \n",
    "The reward map is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 7\n",
    "W = 8\n",
    "reward_map = np.zeros(shape=(H,W))\n",
    "\n",
    "reward_map[0,:] = -1.\n",
    "reward_map[6,:] = -1.\n",
    "reward_map[3,4] = 5.\n",
    "reward_map[3,7] = 10.\n",
    "\n",
    "vis_environment(reward_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find the function <tt>plot_training</tt> that plots reward and correction. \n",
    "You will not have to modify this function, but read the docstring to familiarize yourself with how it can be used. \n",
    "Since the values change with very high frequency <tt>plot_training</tt> applies moving-average smoothing to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(rewards, corrections, title=None):\n",
    "    \"\"\"\n",
    "    Plot curves for reward and correction based on Q-learning episodes.\n",
    "    The curves are smoothed using a moving-average window.\n",
    "    \n",
    "    Args:\n",
    "        rewards: a (numpy) array containing the received reward for each episode\n",
    "        corrections: a (numpy) array containing the temporal difference correction term for each episode\n",
    "        title (optional): title to display above the plots\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    n = 100\n",
    "    \n",
    "    cummulative_reward = np.cumsum(rewards)\n",
    "    reward_ma = (cummulative_reward[n:] - cummulative_reward[:-n])/n\n",
    "    \n",
    "    ax[0].plot(reward_ma)\n",
    "    ax[0].set_title(\"Reward\", fontsize=15)\n",
    "    ax[0].set_xlabel(\"Episode\")\n",
    "    ax[0].set_ylabel(\"Reward\")\n",
    "    \n",
    "    cummulative_corr = np.cumsum(corrections)\n",
    "    corr_ma = (cummulative_corr[n:] - cummulative_corr[:-n])/n\n",
    "    ax[1].plot(corr_ma)\n",
    "    ax[1].set_title(\"Correction\", fontsize=15)\n",
    "    ax[1].set_xlabel(\"Episode\")\n",
    "    ax[1].set_ylabel(\"Correction\")\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=15)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:orange;margin-bottom:0px;\">Task 2.1: Changing $\\gamma$ and $\\epsilon$</h2>\n",
    "\n",
    "Let's now investigate how the $\\gamma$ and $\\epsilon$ parameters impact the learned policy.\n",
    "\n",
    "* Fix $\\epsilon = 0.5$ and for each $\\gamma$ in $\\{0.5, 0.75, 0.95\\}$ train for 30000 episodes (this might take a while). Remember to reset the Q-table between runs.\n",
    "* For each value of $\\gamma$, use <tt>plot_training</tt> to plot the reward and correction and use <tt>vis_environment</tt> to visualize a greedy policy based on the learned Q-table.\n",
    "* Repeat with $\\epsilon = 0.1$.\n",
    "* Explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "Your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment C\n",
    "This is a smaller, $3 \\times 6$ environment. \n",
    "Here the agent starts each episode in the state $(2,0)$.\n",
    "The reward map is defined below.\n",
    "In the following task we will experiment with different values of $\\beta$ for this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 3\n",
    "W = 6\n",
    "reward_map = np.zeros(shape=(H,W))\n",
    "\n",
    "reward_map[2,1:5] = -1.\n",
    "reward_map[2,5] = 10.\n",
    "\n",
    "vis_environment(reward_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:orange;margin-bottom:0px;\">Task 2.3: Probabilistic Transition Model</h2>\n",
    "\n",
    "Let's investigate how different values of $\\beta$ impact what the agent learns. For this experiment set $\\gamma = 0.6$.\n",
    "\n",
    "* For each value of $\\beta$ in {0, 0.2, 0.4, 0.66} train the agent 10000 episodes using an $\\epsilon$-greedy policy with $\\epsilon = 0.5$. Remember to reset the Q-table between runs.\n",
    "* Use <tt>vis_environment</tt> to visualize a greedy policy based on the learned Q-table for each of the values of $\\beta$.\n",
    "* Explain how the greedy policy changes with different values of $\\beta$ and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "    \n",
    "Your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: REINFORCE\n",
    "In previous sections the policy was implicitly defined by estimated Q-values. \n",
    "Now we will take a different approach and learn the policy directly using function estimation.\n",
    "For this we look to the area of policy gradient methods.\n",
    "In particular we will implement the REINFORCE algorithm.\n",
    "\n",
    "Recall that in policy gradient methods we consider the policy $\\pi(A|S, \\theta)$ to be a distribution over the action space, parameterized by $\\theta$ and conditioned on a state $S$. \n",
    "The idea is then to optimize the policy using gradient ascent.\n",
    "Let the performance of the policy parametrized by $\\theta$ be \n",
    "$$\n",
    "J(\\theta) = v_{\\pi_\\theta}(s_0)\n",
    "$$\n",
    "that is, the value function of the starting state.\n",
    "We are then interested in the gradient of $J$ w.r.t. $\\theta$, which can be shown to be\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi}\\left[G_t \\nabla_\\theta \\ln(\\pi(A_t|S_t, \\theta))\\right]\n",
    "$$\n",
    "where\n",
    "$$\n",
    "G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t-1}R_k\n",
    "$$\n",
    "for an episode of length $T$. \n",
    "\n",
    "The REINFORCE algorithm approximates the expectation in the gradient by sampling from the policy and performing stochastic gradient ascent. \n",
    "The sampling is performed by following the policy $\\pi_\\theta$ for one episode (\"rolling out\" the policy). \n",
    "This results in a trajectory, a sequence of states, actions and rewards on the form $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T$.\n",
    "Gradient ascent is then performed using the update rule\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla_\\theta \\ln(\\pi(A_t |S_t, \\theta))\n",
    "$$\n",
    "for each $t = 0, 1, \\dots, T-1$, where $\\alpha$ is the learning rate.\n",
    "Note that all future rewards are needed to compute $G_t$, so the entire episode has to be completed before any gradient ascent step can be taken.\n",
    "\n",
    "We want to implement REINFORCE by parametrizing $\\pi_\\theta$ as a neural network. $\\theta$ is then all trainable weights in the network. In supervised settings neural networks are often trained with stochastic gradient descent (or some extension thereof), following the update rule\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha w \\nabla_\\theta L(y, \\hat{y})\n",
    "$$\n",
    "where $L$ is the loss function, $\\alpha$ the learning rate, $y$ the ground truth and $\\hat{y}$ the network output. \n",
    "$w$ is a sample-specific weight, usually assumed to just be 1 (all training samples are equally important).\n",
    "Note that this is very similar to the REINFORCE update rule and in fact we can get exactly the REINFORCE update by:\n",
    "* Using a cross-entropy loss $L(y, \\hat{y}) = - \\sum_i y_i \\ln(\\hat{y}_i)$\n",
    "* Letting $y$ be a vector with 1 at the position corresponding to $A_t$ and zero everywhere else (\"one-hot\"-encoded).\n",
    "* Feeding $S_t$ as input to the network.\n",
    "* Setting the sample weight $w = \\gamma^t G_t$.\n",
    "\n",
    "Keeping in mind that the network outputs $\\hat{y}_a = \\pi(a | S_t, \\theta)$, the choices above result in the network being trained with the REINFORCE update rule (insert the values into the stochastic gradient descent formulation to see this).\n",
    "To speed up the computation we will also use batch gradient descent instead of training on each state-action pair individually.\n",
    "\n",
    "Throughout this section we use a deterministic transition model ($\\beta = 0$) and let $\\gamma = 0.95$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment D\n",
    "For this section we consider a $4 \\times 4$ grid. \n",
    "This time around we want to teach the agent to navigate to a goal in **any** position.\n",
    "The agent will start in a random position and it will be told the position of the goal.\n",
    "The agent receives a reward of 5 when it reaches the goal.\n",
    "\n",
    "In order to make the agent adapt its policy to reach the goal we need a way to tell the agent where the goal is. \n",
    "Since our agent doesn't have any memory mechanism, we provide the goal coordinates as part of the state at every timestep.\n",
    "This increases the dimensionality of our state space to 4, now consisting of vectors $(y_{\\text{agent}}, x_{\\text{agent}}, y_{\\text{goal}}, x_{\\text{goal}})$. The actions of the agent can however only impact its own position, so there is no way to move to a part of the state space with different values of $y_{\\text{goal}}$ and $x_{\\text{goal}}$ in a single episode. Note the following:\n",
    "* The agent has to discover that the so-called goal position receives the highest reward, which makes it the destination.\n",
    "* The agent has to learn a policy to reach the destination.\n",
    "* The policy has to depend on the destination, because the goal position will vary between episodes. This will make the agent able to reach previously unseen goal positions.\n",
    "\n",
    "Since we only have a single reward we do not specify a reward map. Instead the goal coordinates are given to the neccesary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment size\n",
    "H = 4\n",
    "W = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to be working with a probabilistic policy in this section we will need some new ways of visualizing it. \n",
    "Below you'll find the function <tt>vis_prob</tt> that can be used to visualize Environment D with different goal positions and probabilistic policies.\n",
    "As previously, you will not have to make any changes to this function, but read the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 0.9\n",
    "ARROWS = [\"↑\", \"→\", \"↓\", \"←\"]\n",
    "\n",
    "def vis_prob(goal, policy=None, title=None, size=(H,W)):\n",
    "    \"\"\"\n",
    "    Visualize an environment with a single goal state. \n",
    "    If a policy is given also visualize the chosen action for each state and the probabilities of each action.\n",
    "    \n",
    "    Args:\n",
    "        goal: coordinates of the goal state, array with 2 entries\n",
    "        policy (optional): policy object that implements an act method and a get_action_dist method\n",
    "        title (optional): figure title\n",
    "        size (optional): dimensions of the environment, in the form (H,W)\n",
    "    \"\"\"\n",
    "   \n",
    "    fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "    \n",
    "    tiles = []\n",
    "    size_y, size_x = size\n",
    "   \n",
    "    for i_x in range(size_x): \n",
    "        for i_y in range(size_y): \n",
    "            x = i_x - 0.5*TILE_SIZE\n",
    "            y = i_y - 0.5*TILE_SIZE\n",
    "            \n",
    "            is_goal = (i_x == goal[1]) and (i_y == goal[0])\n",
    "            tile_color = (1.0,1.0,1.0)\n",
    "            \n",
    "            if is_goal:\n",
    "                # Write out reward\n",
    "                ax.text(i_x, i_y, s=\"G\", \n",
    "                    verticalalignment=\"center\", horizontalalignment=\"center\", fontsize=12) # reward\n",
    "                tile_color = (0.0,1.0,0.0)\n",
    "                \n",
    "            elif policy:\n",
    "                # Show policy decisions\n",
    "                action_dist = policy.get_action_dist(i_y, i_x, *goal)\n",
    "                action = np.argmax(action_dist)\n",
    "                ax.text(i_x, i_y, s=ARROWS[action], verticalalignment=\"center\", \n",
    "                        horizontalalignment=\"center\", fontsize=30)\n",
    "                \n",
    "                # Direction probabilities\n",
    "                ax.text(i_x, i_y-0.5*TILE_SIZE, s=\"{:.2f}\".format(action_dist[0]), \n",
    "                        verticalalignment=\"top\", horizontalalignment=\"center\", fontsize=12) # up\n",
    "                ax.text(i_x, i_y+0.5*TILE_SIZE, s=\"{:.2f}\".format(action_dist[2]), \n",
    "                        verticalalignment=\"bottom\", horizontalalignment=\"center\", fontsize=12) # down\n",
    "                ax.text(i_x-0.5*TILE_SIZE, i_y, s=\"{:.2f}\".format(action_dist[3]), \n",
    "                        verticalalignment=\"center\", horizontalalignment=\"left\", fontsize=12) # left\n",
    "                ax.text(i_x+0.5*TILE_SIZE, i_y, s=\"{:.2f}\".format(action_dist[1]), \n",
    "                        verticalalignment=\"center\", horizontalalignment=\"right\", fontsize=12) # right\n",
    "           \n",
    "            tiles.append(patch.Rectangle((x,y),TILE_SIZE,TILE_SIZE, facecolor=tile_color))\n",
    "   \n",
    "    tile_coll = coll.PatchCollection(tiles, match_original=True, edgecolor=\"#787878\")\n",
    "    ax.add_collection(tile_coll)\n",
    "    ax.set_xlim(-0.5, size_x-0.5)\n",
    "    ax.set_ylim(-0.5, size_y-0.5)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title, fontsize=15)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the policy network using [keras](https://keras.io/). \n",
    "Below you will find the definition of a small neural network with two hidden layers of 32 units each. \n",
    "The network takes 4 inputs (agent coordinates and goal coordinates) and has 4 outputs, allowing it to represent a distribution over the possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(32, input_shape=(4,)),\n",
    "    layers.Dense(32),\n",
    "    layers.Dense(4),\n",
    "    layers.Softmax(),\n",
    "])\n",
    "\n",
    "opt = optimizers.SGD(learning_rate=0.001) # Keep learning rate at 0.001\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\")\n",
    "\n",
    "init_weights = model.get_weights() # Save initial weights for resetting later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:orange;margin-bottom:0px;\">Task 3.1: Deep Policy</h2>\n",
    "\n",
    "To make it easy to work with, we wrap our neural network in a policy class. \n",
    "Below you will find the outline of the <tt>DeepPolicy</tt> class. \n",
    "Similarly to the earlier policies it has an <tt>act</tt> method, but <tt>DeepPolicy</tt> also has functions for getting the estimated distribution over actions (<tt>get_action_dist</tt>) and training the network on a rolled out trajectory (<tt>train</tt>).\n",
    "\n",
    "* Implement <tt>get_action_dist</tt> according to its docstring.\n",
    "* Implement <tt>act</tt> according to its docstring.\n",
    "* Implement <tt>train</tt> according to its docstring.\n",
    "\n",
    "Note that you can use the [predict](https://keras.io/models/model/#predict) method on the keras model to perform one forward pass through the network. You can use the [train_on_batch](https://keras.io/models/model/#train_on_batch) method on the keras model to train the network. Remember to use the <tt>sample_weight</tt> argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPolicy:\n",
    "    \"\"\"\n",
    "    Policy for acting using a deep neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, gamma=0.95):\n",
    "        \"\"\"\n",
    "        Initialize a new DeepPolicy acting with respect to the given model.\n",
    "        \n",
    "        Args:\n",
    "            model: keras model to use for estimating action probabilities\n",
    "            gamma (optional): dicounting factor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.network = model\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_action_dist(self, y, x, goal_y, goal_x):\n",
    "        \"\"\"\n",
    "        Get the estimated distribution over actions for the given state description.\n",
    "        \n",
    "        Args:\n",
    "            y: agent y coordinate\n",
    "            x: agent x coordinate\n",
    "            goal_y: goal y coordinate\n",
    "            goal_x: goal x coordinate\n",
    "            \n",
    "        Returns:\n",
    "            A 4-entry array containing the probabilities of each action.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        net_input = np.array([[y, x, goal_y, goal_x]])\n",
    "        return self.network.predict(net_input)[0]\n",
    "        \n",
    "    def act(self, y, x, goal_y, goal_x):\n",
    "        \"\"\"\n",
    "        Get an action for the given state from the policy.\n",
    "        \n",
    "        Args:\n",
    "            y: agent y coordinate\n",
    "            x: agent x coordinate\n",
    "            goal_y: goal y coordinate\n",
    "            goal_x: goal x coordinate\n",
    "            \n",
    "        Returns:\n",
    "             An action (integer in {0,1,2,3}) sampled from the estimated action distribution.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        # Hint: Use get_action_dist\n",
    "        action_dist = self.get_action_dist(y, x, goal_y, goal_x)\n",
    "        \n",
    "        # Sample from categorical distribution\n",
    "        action = np.random.choice(np.arange(4), p=action_dist)\n",
    "        return action\n",
    "    \n",
    "    def train(self, states, actions, rewards, goal_y, goal_x):\n",
    "        \"\"\"\n",
    "        Train the policy network on a rolled out trajectory.\n",
    "        States, actions and rewards must all have the same length.\n",
    "        \n",
    "        Args:\n",
    "            states: array of states visited throughout the trajectory, the first entry being the starting state\n",
    "            actions: array of actions taken throughout the trajectory\n",
    "            rewards: array of rewards received throughout the trajectory\n",
    "            goal_y: goal y coordinate\n",
    "            goal_x: goal x coordinate\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        length = actions.size\n",
    "        \n",
    "        # Construct batch for training\n",
    "        inputs = np.append(states, \n",
    "                           np.repeat([[goal_y, goal_x]], length, axis=0), \n",
    "                           axis=1)\n",
    "        targets = utils.to_categorical(actions, num_classes=4)\n",
    "        \n",
    "        # Sample weights \n",
    "        discounting = np.power(self.gamma, np.arange(length))\n",
    "        reward_matrix = lin.hankel(rewards)\n",
    "        weighting = reward_matrix@discounting\n",
    "        \n",
    "        # Train on batch\n",
    "        self.network.train_on_batch(x=inputs, y=targets, \n",
    "                                    sample_weight=weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll instantiate our newly implemented <tt>DeepPolicy</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_policy = DeepPolicy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Q-learning we will use a function to run one episode of training in the environment. \n",
    "This time the function <tt>reinforce_episode</tt> below is completely implemented for you.\n",
    "It is very similar to <tt>q_learning</tt>, but note that we here train on entire trajectories rather than single state transitions.\n",
    "This means that all training happens at the end of each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_episode(policy, goal, beta=0.0):\n",
    "    \"\"\"\n",
    "    Rolls out a trajectory in the environment until the goal is reached.\n",
    "    Then trains the policy using the collected states, actions and rewards.\n",
    "    \n",
    "    Args:\n",
    "        policy: deep policy to follow and train\n",
    "        goal: coordinates of the goal state, array with 2 entries\n",
    "        beta (optional): probability of slipping in the transition model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomize starting position\n",
    "    cur_pos = goal\n",
    "    while np.all(cur_pos == goal):\n",
    "        cur_pos = np.random.randint(4, size=2)\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards =  []\n",
    "    \n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Follow policy\n",
    "        action = policy.act(cur_pos[0], cur_pos[1], *goal)\n",
    "        \n",
    "        # Store state\n",
    "        states.append(cur_pos)\n",
    "\n",
    "        # Take action\n",
    "        new_pos = transition_model(cur_pos, action, beta, H, W)\n",
    "        steps += 1\n",
    "        \n",
    "        # 5 reward if the goal has been reached\n",
    "        reward = int(np.all(new_pos == goal))*5.\n",
    "        \n",
    "        # Prevent getting stuck and/or training on unneccesarily long episodes\n",
    "        if steps >= 200:\n",
    "            break\n",
    "        \n",
    "        # Store Action, Reward\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        cur_pos = new_pos\n",
    "\n",
    "        if reward != 0.:\n",
    "            # Train network\n",
    "            policy.train(np.array(states), np.array(actions), \n",
    "                         np.array(rewards), *goal)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:orange;margin-bottom:0px;\">Task 3.2: Random Goal Position</h2>\n",
    "\n",
    "In this task we will use 8 goal positions for training and then validate on the remaining 8 possible goal positions. \n",
    "Each training episode should use a random goal position from <tt>TRAIN_GOALS</tt>.\n",
    "Much of the code is implemented for you below.\n",
    "\n",
    "* Run 3000 episodes of training using REINFORCE with a random goal position from <tt>TRAIN_GOALS</tt> for each episode. Note that we are now performing back-propagation in a neural network in each episode. Even though the network is small, this is quite computationally heavy and might take some time.\n",
    "* Use the goal positions in <tt>VAL_GOALS</tt> for validation. Visualize the policy using <tt>vis_prob</tt> for these goal positions before and after training.\n",
    "* Answer the questions below:\n",
    "    1. Interpret the different action probabilities. In particular consider states where \n",
    "        1. There are multiple optimal actions.\n",
    "        2. The agent prefers a suboptimal action (if such exists).\n",
    "    2. What would happen if we applied Q-learning to this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TRAIN_GOALS = [(0,0), (0,2), (1,0), (1,3), (2,0), (2,1), (3,1), (3,2),]\n",
    "VAL_GOALS = [(0,1), (0,3), (1,1), (1,2), (2,2), (2,3), (3,0), (3,3),]\n",
    "\n",
    "model.set_weights(init_weights)\n",
    "\n",
    "def show_validation(policy, title):\n",
    "    for g in VAL_GOALS:\n",
    "        vis_prob(g, policy=policy, title=title)   \n",
    "\n",
    "show_validation(deep_policy, \"Before training\")\n",
    "\n",
    "# Your code here\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "Your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:orange;margin-bottom:0px;\">Task 3.3: Training on Top Row</h2>\n",
    "\n",
    "Now let's see what happens if the agent only trains with goals from the same row. \n",
    "In the <tt>TRAIN_GOALS</tt> given below all goal positions are on the top row.\n",
    "We validate on three goal positions on the rows below.\n",
    "We can of course validate the policy on more goal positions, but these three should be enough to draw some conclusions.\n",
    "\n",
    "* Repeat the task 3.2 but with the <tt>TRAIN_GOALS</tt> and <tt>VAL_GOALS</tt> given below.\n",
    "* Answer the questions below:\n",
    "    1. Has the agent learned a good policy? Why / Why not?\n",
    "    2. Compare this with the results from the task 3.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GOALS = [(0,0), (0,1), (0,2), (0,3), ]\n",
    "VAL_GOALS = [(3,0), (1,3), (2,2),]\n",
    "\n",
    "model.set_weights(init_weights)\n",
    "\n",
    "show_validation(deep_policy, \"Before training\")\n",
    "\n",
    "# Your code here\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "Your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "* After you have completed all tasks, hand in the notebook file (Lab4.ipynb) through Lisam.\n",
    "* Make sure that all results that you want to submit are in the notebook (run all cells that produce plots, etc.). \n",
    "* Make sure the submitted code reproduces your results. You can run the entire notebook from the start by selecting *Run -> Run all cells*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tasks\n",
    "If you are interested in experimenting more with reinforcement learning you will find some ideas for extensions to this lab below. \n",
    "These are not part of the course and do not have to be handed in anywhere.\n",
    "\n",
    "1. The REINFORCE implementation in section 3 is quite slow. \n",
    "One reason for this is that each step in the episode requires a forward pass through the network. \n",
    "This could be greatly improved by running multiple episodes in parallel, performing the forward pass on an entire batch of states. \n",
    "2. Try using Q-learning for the randomized goal task in Environment D. Compare it to using REINFORCE. \n",
    "3. The REINFORCE algorithm suffers from high variance. One way to help with this is to introduce a baseline (see section 13.4 in the book). \n",
    "Implement a baseline function parametrized as a second neural network, jointly trained with the policy network.\n",
    "4. Extend the above approach to a full Actor-Critic method.\n",
    "5. Beyond this notebook a nice platform for experimenting with reinforcement learning is [Open AI Gym](https://gym.openai.com/). \n",
    "It includes a number of different environments and can interface with implementations of most well-known RL algorithms. \n",
    "A good place to start is [Spinning up in Deep RL](https://spinningup.openai.com/), which offer both theoretical explanations of many deep RL algorithms and implementations that can be used with the Gym environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
